version: '3'

services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - airflow_network

  minio:
    image: minio/minio
    container_name: minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"   # Порт для S3 совместимого API
      - "9001:9001"   # Порт для консоли управления MinIO
    volumes:
      - ./minio_data:/data    # Хранение данных на хосте
    command: server /data --console-address ":9001"
    networks:
      - airflow_network

  airflow-scheduler:
    image: apache/airflow:latest-python3.9
    container_name: airflow-scheduler
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - PYTHONPATH=/opt/airflow/app
    env_file:
      - .env
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./app:/opt/airflow/app
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: >
      bash -c "
        pip install poetry -i https://mirrors.aliyun.com/pypi/simple/ &&
        cd /opt/airflow/app &&
        poetry install && 
        airflow db init && 
        airflow webserver > /dev/null 2>&1 & 
        airflow scheduler"

    networks:
      - airflow_network
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: "4G"
        reservations:
          cpus: "1"
          memory: "2G"

volumes:
  postgres_data:

networks:
  airflow_network:
    driver: bridge

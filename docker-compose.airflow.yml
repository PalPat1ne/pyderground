version: '3'

services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - airflow_network

  minio:
    image: minio/minio
    container_name: minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"   # Порт для S3 совместимого API
      - "9001:9001"   # Порт для консоли управления MinIO
    volumes:
      - ./minio_data:/data    # Хранение данных на хосте
    command: server /data --console-address ":9001"
    networks:
      - airflow_network


  airflow-webserver:
    image: apache/airflow:latest-python3.9
    container_name: airflow-webserver
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    restart: always
    depends_on:
      - airflow-scheduler
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - PYTHONPATH=/opt/airflow/app
    env_file:
      - .env
    volumes:
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: >
      bash -c "airflow webserver"
    networks:
      - airflow_network
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: "4G"
        reservations:
          cpus: "1"
          memory: "2G"

  airflow-scheduler:
    image: apache/airflow:latest-python3.9
    container_name: airflow-scheduler
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    restart: always
    depends_on:
      - airflow-init
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - PYTHONPATH=/opt/airflow/app
    env_file:
      - .env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./app:/opt/airflow/app
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: >
      bash -c "
        pip install poetry -i https://mirrors.aliyun.com/pypi/simple/ &&
        cd /opt/airflow/app &&
        poetry install &&
        airflow scheduler"
    networks:
      - airflow_network
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: "4G"
        reservations:
          cpus: "1"
          memory: "2G"

  airflow-init:
    image: apache/airflow:latest-python3.9
    container_name: airflow-init
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - PYTHONPATH=/opt/airflow/app
    env_file:
      - .env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./app:/opt/airflow/app
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: >
      bash -c "airflow db init"
    networks:
      - airflow_network

volumes:
  postgres_data:

networks:
  airflow_network:
    driver: bridge
